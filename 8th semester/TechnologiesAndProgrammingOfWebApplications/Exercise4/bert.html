<!DOCTYPE html>
<html lang="el">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Άσκηση 4</title>
        <link rel="stylesheet" href="css/exercise4.css">
    </head>
    <body class="page">

        <header><h1>Άσκηση 4</h1></header>
        
        <nav >
            <ul class='menu'>
                <li><a href="index.html">Τεχνητή Νοημοσύνη</a></li>
                <li><a href="main_content.html"> Μηχανική Μάθηση και Βαθιά Μάθηση </a></li>
                <li><a href="bert.html"> BERT</a></li>
            </ul>
        </nav>
        
        <aside>
            <ul>
                <li>"Robots are not going to replace humans, they are going to make their jobs much more humane. Difficult, demeaning, demanding, dangerous, dull – these are the jobs robots will be taking." Sabine Hauert, Co-founder of Robohub.org</li>
                <li>"The coming era of Artificial Intelligence will not be the era of war, but be the era of deep compassion, non-violence, and love." Amit Ray, Pioneer of Compassionate AI Movement.</li>
            </ul>
        </aside>

        <main class='container'> 
            <section >
                <h2>BERT</h2>
                <article>
                    <h3> Τι είναι το BERT;</h3>
                    Έχοντας κάνει μια καλή εισαγωγή στο τι εστί τεχνητή νοημοσύνη, θα δούμε ένα σύγχρονο μοντέλο, το οποίο ονομάζεται BERT.
                    Το BERT (Bidirectional Encoder Representations from Transformers) είναι μία Transformer-based machine learning τεχνική η οποία δημιουργήθηκε από τη Google το 2018 και είναι state-of-the-art σε πολλά tasks επεξεργασίας φυσικής γλώσσας (NLP).
                    Χρησιμοποιεί Transformer, έναν attention-μηχανισμό ο οποίος μαθαίνει σχέσεις μεταξύ των λέξεων σε ένα κείμενο.
                    Δεδομένου ότι o Transformer αποτελείται από δύο μηχανισμούς, ένα encoder όπου και εισάγεται το input και έναν decoder για το output, το BERT χρησιμοποιεί μόνο τον encoder μηχανισμό. 
                </article>
            </section>  
            <section>
                <h2> Περισσότερες Λεπτομέρειες για το BERT:</h2>
                <figure>
                    <img src="images/bert.PNG" alt="bert" width="300" height="500">
                    <figcaption> Figure 1 - Bert.</figcaption>
                </figure>
                <article>
                    <h3>Λεπτομέρειες Αρχιτεκτονικής του BERT:</h3>
                    Υπάρχουν 2 BERT αρχιτεκτονικές: BERT Base (12 layers (transformer blocks), 12 attention heads, and 110 million parameters) και BERT Large ( 24 layers (transformer blocks), 16 attention heads and, 340 million parameters), Όλα αυτά τα transformer layers είναι, όπως αναφέραμε και παραπάνω, είναι μόνο encoders. 
                </article>
            </section>

        </main>

        <footer>
            <p>Δημιουργός: Χρυσούλα Δικονιμάκη</p>
            <p>Επικοινωνία: p3170039@aueb.gr </p>
        </footer>

    </body>
</html>